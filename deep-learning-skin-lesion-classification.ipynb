{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal\n",
    "The goal is to make a simple model that can go from an image (taken with a smartphone) to a prediction of how likely different skin-conditions are based on a picture of your skin. It is not designed for medical use and serves as a fun toy to see how image processing works (and fails) in real conditions.\n",
    "## Setup\n",
    "We basically take a pretrained model (MobileNet in this case) and add a few layers that we train ourselves in order to determine if the food contains any of the 7 different conditions. We try to create a balanced training group and a realistic validation group to know if the model is learning anything useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "import warnings # tf needs to learn to stfu\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "warnings.simplefilter(action=\"ignore\", category=UserWarning)\n",
    "warnings.simplefilter(action=\"ignore\", category=RuntimeWarning)\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatplotlib\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minline\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mrcParams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfigure.figsize\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mrcParams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfigure.dpi\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m125\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.rcParams[\"figure.figsize\"] = (15, 10)\n",
    "plt.rcParams[\"figure.dpi\"] = 125\n",
    "plt.rcParams[\"font.size\"] = 14\n",
    "plt.rcParams['font.family'] = ['sans-serif']\n",
    "plt.rcParams['font.sans-serif'] = ['DejaVu Sans']\n",
    "plt.style.use('ggplot')\n",
    "sns.set_style(\"whitegrid\", {'axes.grid': False})\n",
    "plt.rcParams['image.cmap'] = 'gray' # grayscale looks better\n",
    "from itertools import cycle\n",
    "prop_cycle = plt.rcParams['axes.prop_cycle']\n",
    "colors = prop_cycle.by_key()['color']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from skimage.io import imread as imread\n",
    "from skimage.util import montage\n",
    "montage_rgb = lambda x: np.stack([montage(x[:, :, :, i]) for i in range(x.shape[3])], -1)\n",
    "from skimage.color import label2rgb\n",
    "image_dir = Path('..') / 'input' / 'skin-cancer-mnist-ham10000'\n",
    "mapping_file = Path('..') / 'input' / 'skin-images-to-features' / 'color_features.json'\n",
    "skin_df = pd.read_json(mapping_file)\n",
    "#skin_df['image_path'] = skin_df['image_path'].map(lambda x: image_dir / 'subset' / x) \n",
    "print(skin_df['image_path'].map(lambda x: Path(x).exists()).value_counts())\n",
    "skin_df.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split up the groups so we can validate our model on something besides the direct training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "raw_train_df, valid_df = train_test_split(skin_df, \n",
    "                 test_size = 0.3, \n",
    "                  # hack to make stratification work                  \n",
    "                 stratify = skin_df['dx_id'])\n",
    "print(raw_train_df.shape[0], 'training masks')\n",
    "print(valid_df.shape[0], 'validation masks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAUSSIAN_NOISE = 0.05\n",
    "# number of validation images to use\n",
    "VALID_IMG_COUNT = 1500\n",
    "BASE_MODEL='MobileNet' # ['VGG16', 'RESNET52', 'InceptionV3', 'Xception', 'DenseNet169', 'DenseNet121']\n",
    "IMG_SIZE = (224, 224) # [(224, 224), (384, 384), (512, 512), (640, 640)]\n",
    "BATCH_SIZE = 64 # [1, 8, 16, 24]\n",
    "DROPOUT = 0.5\n",
    "DENSE_COUNT = 256\n",
    "SAMPLE_PER_GROUP = 2200\n",
    "LEARN_RATE = 2e-4\n",
    "EPOCHS = 25\n",
    "FLATTEN = True\n",
    "RGB_FLIP = 1 # should rgb be flipped when rendering images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = raw_train_df.\\\n",
    "     groupby('dx_name').\\\n",
    "     apply(lambda x: x.sample(SAMPLE_PER_GROUP//2, replace=True)).\\\n",
    "     reset_index(drop=True)\n",
    "train_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare for Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "if BASE_MODEL == 'MobileNet':\n",
    "    from keras.applications.mobilenet_v2 import MobileNetV2 as PTModel, preprocess_input\n",
    "elif BASE_MODEL=='VGG16':\n",
    "    from keras.applications.vgg16 import VGG16 as PTModel, preprocess_input\n",
    "elif BASE_MODEL=='RESNET52':\n",
    "    from keras.applications.resnet50 import ResNet50 as PTModel, preprocess_input\n",
    "elif BASE_MODEL=='InceptionV3':\n",
    "    from keras.applications.inception_v3 import InceptionV3 as PTModel, preprocess_input\n",
    "elif BASE_MODEL=='Xception':\n",
    "    from keras.applications.xception import Xception as PTModel, preprocess_input\n",
    "elif BASE_MODEL=='DenseNet169': \n",
    "    from keras.applications.densenet import DenseNet169 as PTModel, preprocess_input\n",
    "elif BASE_MODEL=='DenseNet121':\n",
    "    from keras.applications.densenet import DenseNet121 as PTModel, preprocess_input\n",
    "else:\n",
    "    raise ValueError('Unknown model: {}'.format(BASE_MODEL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "dg_args = dict(featurewise_center = False, \n",
    "                  samplewise_center = False,\n",
    "                  rotation_range = 45, \n",
    "                  width_shift_range = 0.1, \n",
    "                  height_shift_range = 0.1, \n",
    "                  shear_range = 0.01,\n",
    "                  zoom_range = [0.9, 1.25],  \n",
    "                  brightness_range = [0.7, 1.3],\n",
    "                   \n",
    "                  horizontal_flip = True, \n",
    "                  vertical_flip = False,\n",
    "                  fill_mode = 'reflect',\n",
    "                   data_format = 'channels_last',\n",
    "              preprocessing_function = preprocess_input)\n",
    "\n",
    "valid_args = dict(fill_mode = 'reflect',\n",
    "                   data_format = 'channels_last',\n",
    "                  preprocessing_function = preprocess_input)\n",
    "\n",
    "core_idg = ImageDataGenerator(**dg_args)\n",
    "valid_idg = ImageDataGenerator(**valid_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flow_from_dataframe(img_data_gen, raw_df, path_col, y_col, **dflow_args):\n",
    "    \"\"\"Keras update makes this much easier\"\"\"\n",
    "    in_df = raw_df.copy()\n",
    "    in_df[path_col] = in_df[path_col].map(str)\n",
    "    in_df[y_col] = in_df[y_col].map(lambda x: np.array(x))\n",
    "    df_gen = img_data_gen.flow_from_dataframe(in_df, \n",
    "                                              x_col=path_col,\n",
    "                                              y_col=y_col,\n",
    "                                    class_mode = 'raw',\n",
    "                                    **dflow_args)\n",
    "    # posthoc correction\n",
    "    df_gen._targets = np.stack(df_gen.labels, 0)\n",
    "    return df_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = flow_from_dataframe(core_idg, train_df, \n",
    "                             path_col = 'image_path',\n",
    "                            y_col = 'dx_id', \n",
    "                            target_size = IMG_SIZE,\n",
    "                             color_mode = 'rgb',\n",
    "                            batch_size = BATCH_SIZE)\n",
    "\n",
    "# used a fixed dataset for evaluating the algorithm\n",
    "valid_x, valid_y = next(flow_from_dataframe(valid_idg, \n",
    "                               valid_df, \n",
    "                             path_col = 'image_path',\n",
    "                            y_col = 'dx_id', \n",
    "                            target_size = IMG_SIZE,\n",
    "                             color_mode = 'rgb',\n",
    "                            batch_size = VALID_IMG_COUNT)) # one big batch\n",
    "print(valid_x.shape, valid_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_x, t_y = next(train_gen)\n",
    "print('x', t_x.shape, t_x.dtype, t_x.min(), t_x.max())\n",
    "print('y', t_y.shape, t_y.dtype, t_y.min(), t_y.max())\n",
    "fig, (ax1) = plt.subplots(1, 1, figsize = (10, 10))\n",
    "ax1.imshow(montage_rgb((t_x-t_x.min())/(t_x.max()-t_x.min()))[:, :, ::RGB_FLIP])\n",
    "ax1.set_title('images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the labels\n",
    "Here we show the labels for the batch items and can see how frequent each one is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(t_y.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_pretrained_model = PTModel(input_shape =  t_x.shape[1:], \n",
    "                              include_top = False, weights = 'imagenet')\n",
    "base_pretrained_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models, layers\n",
    "from keras.optimizers import Adam\n",
    "img_in = layers.Input(t_x.shape[1:], name='Image_RGB_In')\n",
    "img_noise = layers.GaussianNoise(GAUSSIAN_NOISE)(img_in)\n",
    "pt_features = base_pretrained_model(img_noise)\n",
    "pt_depth = base_pretrained_model.get_output_shape_at(0)[-1]\n",
    "bn_features = layers.BatchNormalization()(pt_features)\n",
    "feature_dropout = layers.SpatialDropout2D(DROPOUT)(bn_features)\n",
    "if FLATTEN:\n",
    "    flat_layer = layers.Flatten()(bn_features)\n",
    "    collapsed_layer = layers.Dropout(DROPOUT)(flat_layer)\n",
    "else:\n",
    "    collapsed_layer = layers.GlobalAvgPool2D()(bn_features)\n",
    "dr_steps = layers.Dropout(DROPOUT)(layers.Dense(DENSE_COUNT, activation = 'relu')(collapsed_layer))\n",
    "out_layer = layers.Dense(train_df['dx_id'].max()+1, activation = 'softmax')(dr_steps)\n",
    "\n",
    "skin_model = models.Model(inputs = [img_in], outputs = [out_layer], name = 'full_model')\n",
    "\n",
    "skin_model.compile(optimizer = Adam(lr=LEARN_RATE), \n",
    "                   loss = 'sparse_categorical_crossentropy',\n",
    "                   metrics = ['sparse_categorical_accuracy'])\n",
    "\n",
    "skin_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "weight_path=\"{}_weights.best.hdf5\".format('skin_cancer_detector')\n",
    "\n",
    "checkpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1, \n",
    "                             save_best_only=True, mode='min', save_weights_only = True)\n",
    "\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=10, verbose=1, mode='auto', epsilon=0.0001, cooldown=5, min_lr=0.0001)\n",
    "early = EarlyStopping(monitor=\"val_loss\", \n",
    "                      mode=\"min\", \n",
    "                      patience=15) # probably needs to be more patient, but kaggle time is limited\n",
    "callbacks_list = [checkpoint, early, reduceLROnPlat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "train_gen.batch_size = BATCH_SIZE\n",
    "fit_results = skin_model.fit_generator(train_gen, \n",
    "                            steps_per_epoch = train_gen.samples//BATCH_SIZE,\n",
    "                      validation_data = (valid_x, valid_y), \n",
    "                      epochs = EPOCHS, \n",
    "                      callbacks = callbacks_list,\n",
    "                      workers = 3)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (20, 10))\n",
    "ax1.plot(fit_results.history['loss'], label='Training')\n",
    "ax1.plot(fit_results.history['val_loss'], label='Validation')\n",
    "ax1.legend()\n",
    "ax1.set_title('Loss')\n",
    "ax2.plot(fit_results.history['sparse_categorical_accuracy'], label='Training')\n",
    "ax2.plot(fit_results.history['val_sparse_categorical_accuracy'], label='Validation')\n",
    "ax2.legend()\n",
    "ax2.set_title('Binary Accuracy')\n",
    "ax2.set_ylim(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skin_model.load_weights(weight_path)\n",
    "skin_model.save('full_skin_cancer_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in zip(skin_model.metrics_names, \n",
    "        skin_model.evaluate(valid_x, valid_y)):\n",
    "    if k!='loss':\n",
    "        print('{:40s}:\\t{:2.1f}%'.format(k, 100*v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dx_lookup_dict = train_df[['dx_name', 'dx_id']].drop_duplicates().set_index('dx_id').to_dict()['dx_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_x, t_y = next(train_gen)\n",
    "t_yp = skin_model.predict(t_x)\n",
    "fig, (m_axs) = plt.subplots(4, 4, figsize = (20, 20))\n",
    "for i, c_ax in enumerate(m_axs.flatten()):\n",
    "    c_ax.imshow(((t_x[i]-t_x.min())/(t_x.max()-t_x.min()))[:, ::RGB_FLIP])\n",
    "    c_title = '{}\\nPred: {:2.1f}%'.format(dx_lookup_dict[t_y[i]], 100*t_yp[i, t_y[i]])\n",
    "    c_ax.set_title(c_title)\n",
    "    c_ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Data Results\n",
    "Here we show the results on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_x, t_y = valid_x, valid_y\n",
    "t_yp = skin_model.predict(t_x, batch_size=8, verbose=True)\n",
    "fig, (m_axs) = plt.subplots(4, 4, figsize = (20, 20))\n",
    "for i, c_ax in enumerate(m_axs.flatten()):\n",
    "    c_ax.imshow(((t_x[i]-t_x.min())/(t_x.max()-t_x.min()))[:, ::RGB_FLIP])\n",
    "    c_title = '{}\\nPred: {:2.1f}%'.format(dx_lookup_dict[t_y[i]], 100*t_yp[i, t_y[i]])\n",
    "    c_ax.set_title(c_title)\n",
    "    c_ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed Performance by group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "skin_conditions = [dx_lookup_dict[k][:20] for k in range(len(dx_lookup_dict))]\n",
    "t_y_ohe = ohe.fit_transform(t_y.reshape(-1, 1))\n",
    "pred_df = pd.concat([\n",
    "    pd.DataFrame(t_yp, columns=skin_conditions).assign(source='Prediction').assign(id=range(t_yp.shape[0])),\n",
    "    pd.DataFrame(t_y_ohe, columns=skin_conditions).assign(source='Ground-truth').assign(id=range(t_yp.shape[0]))\n",
    "])\n",
    "flat_pred_df = pd.melt(pred_df, id_vars=['source', 'id']).pivot_table(index=['id', 'variable'], columns='source', values='value').reset_index()\n",
    "flat_pred_df['Ground-truth'] = flat_pred_df['Ground-truth'].map(lambda x: 'Positive' if x>0.5 else 'Negative')\n",
    "sns.catplot(data=flat_pred_df, x='Ground-truth', y='Prediction', col='variable', kind='swarm',  col_wrap=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(1, 1, figsize=(12, 5))\n",
    "sns.swarmplot(data=flat_pred_df, hue='Ground-truth', y='Prediction', x='variable', size=2.0, ax=ax1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(1, 1, figsize=(12, 5))\n",
    "sns.boxplot(data=flat_pred_df, hue='Ground-truth', y='Prediction', x='variable', ax=ax1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class-level ROC curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "fig, ax1 = plt.subplots(1, 1, figsize=(10, 10))\n",
    "for i, c_all in enumerate(skin_conditions):\n",
    "    tpr, fpr, thresh = roc_curve(y_true=t_y_ohe[:, i], y_score=t_yp[:, i])\n",
    "    auc_roc = roc_auc_score(y_true=t_y_ohe[:, i], y_score=t_yp[:, i])\n",
    "    ax1.plot(tpr, fpr, '.-', label='{} (AUC:{:2.1%})'.format(c_all, auc_roc), lw=2)\n",
    "ax1.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export the Model\n",
    "We can export the model to tensorflowjs to build a web-app that can automatically predict what allergens are in a given image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skin_model.get_input_at(0), skin_model.get_output_at(0)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 54339,
     "sourceId": 104884,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 26767459,
     "sourceType": "kernelVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelInstanceId": 2507,
     "sourceId": 3402,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 29282,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
